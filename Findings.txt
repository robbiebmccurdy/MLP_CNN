MLP
MLP1.
Accuracy:
The network's accuracy on 10,000 test images is 50%. We see varied performance across different classes. For instance, it does well in identifying cars, planes, and ships with accuracies of 59.9%, 57.4%, and 64.2% respectively. However, it struggles with cats and birds, showing only 23.7% and 35.6% accuracy. Ideally, we'd aim for around 70% accuracy, but given the current performance, a realistic target is an overall accuracy of about 60%.

MLP2.
For cross-validation in our MLP model, I utilized sklearn's KFold to divide the dataset into 5 parts. The training loop was adjusted to iterate over each fold, creating separate training and validation data loaders. Post training and evaluation, the accuracies were:
Fold 0: 50.38%
Fold 1: 49.94%
Fold 2: 50.72%
Fold 3: 50.92%
Fold 4: 49.69%
Average: 50.33%

MLP3.
To improve accuracy, I increased the epochs from 12 to 25. However, this change didn't significantly impact the overall accuracy, which stayed around 50%. Notably, the accuracy for cats improved to 33.3%, but other classes didn't show much improvement. Next, I experimented with different learning rates and found that while some learning rates, like 5e-4, lowered overall accuracy to 48%, they improved performance in lower-accuracy classes. However, the best balance was achieved with a learning rate of 1e-4.
I also tested dropout and data augmentation strategies. Dropout reduced the overall accuracy significantly, but data augmentation had a positive impact, improving the network's accuracy to 51%.

CNN
CNN1.
Accuracy:
The CNN model showed a promising 61% accuracy on 10,000 test images. The model performed consistently across different classes, with the highest accuracies in 'ship' and 'truck' categories at 73.3% and 71.9%. Halfway through the iterations, the loss decreased significantly, suggesting efficient learning.

CNN2.
Applying cross-validation using KFold, the CNN model's accuracies hovered around 59%-61%. The breakdown of accuracies per class was as follows:
Plane: 65%
Car: 71%
Bird: 49%
Cat: 34%
Deer: 66%
Dog: 50%
Frog: 65%
Horse: 70%
Ship: 74%
Truck: 60%

CNN3.
The model maintained an overall accuracy of 60%. Notably, the precision, recall, and F1 score for the 'cat' class were 0.40, 0.34, and 0.37, respectively. These metrics highlight that the model's performance can vary depending on whether we prioritize minimizing false positives (precision) or maximizing true positives (recall).

CC
CC1.
Comparing the MLP and CNN models, the CNN consistently outperformed the MLP with an average accuracy over 60%. This aligns with my expectations as CNNs are generally more adept at handling image data. In my trials, I noticed some underfitting with dropout in the MLP model, leading me to explore other techniques.

PB
PB1.
In the Net class, we have a composition of Conv2d, MaxPool2d, and Linear layers. The total number of parameters, primarily in the convolutional and fully connected layers, adds up to 62,006. This was confirmed using PyTorch's parameters() function, ensuring the accuracy of our manual computation.

PB2.
All 62,006 parameters in the CIFAR-10 CNN's Net class are trainable, located in the Conv2d and Linear layers. These parameters are updated through gradient descent during backpropagation, as confirmed by PyTorch's functionality.

PB3.
The batch_size parameter is crucial in our CIFAR-10 neural network assignments. A smaller batch size, while leading to noisier gradient estimations, can help in avoiding local minima. Conversely, larger batch sizes offer more stable gradients but require more computational resources. The optimal batch size strikes a balance between training stability and resource constraints.

PB4.
In an assignment involving a dataset with 100 binary attributes, a complete binary decision tree would be impractical due to its massive size. A more effective approach focuses on the most influential attribute, a1, which determines the target variable y in most cases. This results in a simpler, more generalizable tree that avoids the pitfalls of overfitting associated with a complete tree structure.