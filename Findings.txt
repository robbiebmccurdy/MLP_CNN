MLP
	MLP1.

		Accuracy: 

			Accuracy of the network on the 10000 test images: 50 %
			Accuracy for class: plane is 57.4 %
			Accuracy for class: car   is 59.9 %
			Accuracy for class: bird  is 35.6 %
			Accuracy for class: cat   is 23.7 %
			Accuracy for class: deer  is 54.4 %
			Accuracy for class: dog   is 41.8 %
			Accuracy for class: frog  is 57.4 %
			Accuracy for class: horse is 56.6 %
			Accuracy for class: ship  is 64.2 %
			Accuracy for class: truck is 55.4 %

		Seems to be a toss-up on whether or not the model can determine things overall. However, upon individual inspection, it seems as though we are
		doing fairly well with finding cars, planes, and ships. The reverse of this though, we are doing extremely poor finding cats and birds. Ideally
		we'd like these numbers to be close to 70% but that may be too optimistic for this model/project. So we will shoot for hopefully an overall
		network accuracy of ~60%.

	MLP2.
		In the MLP model for the CIFAR-10 dataset, incorporating a validation step significantly enhances training effectiveness. Through regular validation checks after each training epoch, I can pinpoint the ideal moment to halt training, helping to prevent overfitting. This is evident when the model performs increasingly well on training data but starts faltering on validation data.

		Moreover, validation insights are key in fine-tuning hyperparameters and adjusting the model's architecture. If the model underperforms on the validation set, it suggests a need for modificationsâ€”perhaps a more complex structure or extended training. Conversely, a pronounced gap between training and validation performance could indicate overfitting, necessitating the introduction of regularization methods. Ultimately, this approach ensures the model's robust generalization, preparing it for effective real-world application.

		Taking this cross-validation into account, our overall accuracy rose to 51% meanwhile many classes rose as well. Most notably, ships rose from 64.2% to 71.1% but a class like deer fell from 54.4% to 42.4% overall.

	MLP3.
		First, to address the accuracy measure, we can try to increase the amount of epochs we iterate against to train our data. I will be increasing the amount of epochs from 12 to 25. Below will be the updated accuracies of 25 epochs. 

		Accuracy of 25 epochs:
			Accuracy of the network on the 10000 test images: 50 %
			Accuracy for class: plane is 53.1 %
			Accuracy for class: car   is 58.8 %
			Accuracy for class: bird  is 29.4 %
			Accuracy for class: cat   is 33.3 %
			Accuracy for class: deer  is 43.5 %
			Accuracy for class: dog   is 41.6 %
			Accuracy for class: frog  is 60.5 %
			Accuracy for class: horse is 59.1 %
			Accuracy for class: ship  is 68.5 %
			Accuracy for class: truck is 55.3 %	

		It seems increasing our epochs from 12 to 25 did very little, if not it seems the overall validation performed slightly worse but essentially the same. A great thing is that a class like cat has gone up in accuracy, but itseems classes around it have either stagnated or fallen.

		Next, I will try adjusting the learning rate. To keep these fair, I will return epochs to 12 instead of 25. Then once we weight what changes the most, at the end I will implement all methods deemed valuable and check their accuracies.

		Accuracy for 5e-4 learning rate: 

			Accuracy of the network on the 10000 test images: 48 %
			Accuracy for class: plane is 48.9 %
			Accuracy for class: car   is 50.8 %
			Accuracy for class: bird  is 35.4 %
			Accuracy for class: cat   is 31.0 %
			Accuracy for class: deer  is 42.0 %
			Accuracy for class: dog   is 44.1 %
			Accuracy for class: frog  is 56.7 %
			Accuracy for class: horse is 52.3 %
			Accuracy for class: ship  is 73.7 %
			Accuracy for class: truck is 53.7 %

		With an updated learning rate, it seems like our overall accuracy has decreased, but upon closer inspection I'd say it seems that our lower end of accuracy has actually improved. The parts that have decreased are mostly all other fields except for ship which has rose to 73.7%. This indicates that potentially learning rate could drive accuracy up, but th is learning rate may be far too great.

		I've tested the learning rates 0.5e-4 and 2.5e-4 as well, and it seems like this value does have an effect but the best overall accuracy has been achieved with 1e-4. The learning rate will be reverted to 1e-4 and another method will be implemented.

		Dropout. I've attempted to add dropout as a mechanic to increase overall accuracy. This mechanic has done nothing but decrerase accuracy. Our overall network accuracy decreased to 44% while numberes went as low as 13.6% for the bird class. Dropout is not a method I would continue to use.

		Data augmentation. Implemented data augmentation that randomly horiziontally flips the data and does a random rotation. Then used it for the training set. This yielded an overall network accuracy of 51% with the highest accuracy being ships at 70.7% and the lowest cats at 29.0% which lines up with other methods just about.

		It seems that data augmentation and increasing the learning rate had the most overall positive impact on the accuracy of the network. Dropout had the largest impact but it was largely negative decreasing our base accuracy from 50% to 44%.

CNN
	CNN1.

		Accuracy: 

			Accuracy of the network on the 10000 test images: 61 %
			Accuracy for class: plane is 66.8 %
			Accuracy for class: car   is 68.8 %
			Accuracy for class: bird  is 43.2 %
			Accuracy for class: cat   is 53.7 %
			Accuracy for class: deer  is 63.9 %
			Accuracy for class: dog   is 39.0 %
			Accuracy for class: frog  is 66.6 %
			Accuracy for class: horse is 66.5 %
			Accuracy for class: ship  is 73.3 %
			Accuracy for class: truck is 71.9 %

			The loss was hitting below 1 even halfway through the iterations. That means that the loss was over half of it's original value by the time we were 5/10 iterations through.

	CNN2.

		To add cross-validation to this CNN model, I imported KFold from sklearn to split the data set into 5 folds. Then modified the training loop to iterate over each fold using a different subset of data for training/validation in each fold. Then inside each loop, data loaders for training and validation are created by basing it off the indicies of the K-Fold. The model is trained, saved, and evaluated on each fold.

		Accuracy: 

			Each fold seemed to hover anywhere from 59%-61% accuracy. The scores at the end were: 

			Accuracy of the network on the 10000 test images: 60 %
			Accuracy of plane : 65 %
			Accuracy of   car : 71 %
			Accuracy of  bird : 49 %
			Accuracy of   cat : 34 %
			Accuracy of  deer : 66 %
			Accuracy of   dog : 50 %
			Accuracy of  frog : 65 %
			Accuracy of horse : 70 %
			Accuracy of  ship : 74 %
			Accuracy of truck : 60 %

	CNN3.

		Accuracy of the network on the 10000 test images: 60 %
		Accuracy of plane : 65 %
		Accuracy of   car : 71 %
		Accuracy of  bird : 49 %
		Accuracy of   cat : 34 %
		Accuracy of  deer : 66 %
		Accuracy of   dog : 50 %
		Accuracy of  frog : 65 %
		Accuracy of horse : 70 %
		Accuracy of  ship : 74 %
		Accuracy of truck : 60 %
		Precision for "cat": 0.40
		Recall for "cat": 0.34
		F1 Score for "cat": 0.37

		It really depends on our goal. If our goal is to minimize the amount of false 'cats' then we'd use precision. If our goal was to get as many 'cats' then we'd use recall. F1 is a balance of these two metrics, so I'd say that one is the most valuable unless our goal is specific.
CC
	CC1.

PB
	PB1.

	PB2.

	PB3.

	PB4.